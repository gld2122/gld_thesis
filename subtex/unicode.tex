\section{Unicode}

\subsection{Computers and Writing Systems}

In the "digital age" has changed writing from a free and individualized practice to
a one based in a discrete and logical structure consisting of discrete glyphs.
Since most early development of computers took place in the United States,
English gained a natural ascendancy over other languages in the field of digital
communication. English, perhaps as a coincidence, is also one of the easiest
languages to represent in code, requiring at the bare minimum just the 26
non-accented characters of the English alphabet, perhaps with some punctuation
and numbers. The ASCII standard encoding, with 127 available code points, is
more than enough to represent the English language in digital form.

As computer technology spread far beyond the labs of Silicon Valley, there was a
need to encode a vast array of linguistic forms, including diacritical marks,
right-to-left scripts, and pictographic writing systems. In the 1990s a plethora
of national encoding schemes were created to represent languages other than
English; the only problem, they were not intercompatible, meaning that documents
could "play nice" when transferred between countries or languages.

Unicode and its dominant encoding scheme UTF-8 intend to alleviate these
problems by collecting all of the world's glyphs into one system. The 127
characters in ASCII are represented using the same single-byte codes, which
means that no conversion for existing ASCII files is necessary, and these will
not become bloated in a new conversion scheme.
